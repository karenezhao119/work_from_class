---
title: "S20 PSTAT126 Final Project"
author: "Karen Zhao"
date: "6/6/2020"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, 
                      eval=T,
                      results='markup', 
                      message=F,
                      warning=F, 
                      fig.align = 'center')

library(readr)
library(dplyr)
library(mlr3)
```

# Regression Analysis on U.S. Life Expectancy

## 1. Introduction 
This project focuses on studying the prediction of life expectancy in the U.S. states based on the dataset ‘state.x77’ in R library, which is derived from the U.S. Department of Commerce,Bureau of the Census (1977) Statistical Abstract of the United States. We will examine the effects of the following 7 variables on life expectancy: population, income, illiteracy, murder rate(Murder), high school graduate rate (HS Grad), land area, and mean number of days with minimum temperature below freezing (Frost). We find that ‘Murder’, ‘HS Grad’, ‘Frost’, and “Population’ are the most related predictors.

```{r}
dat=as.data.frame(state.x77)
attach(dat)
names(dat)
```

## 2. Questions of Interest
Can we predict life expectancy of a region given its population, income, illiteracy, murder rate (Murder), high school graduate percent (HS Grad), land area, and mean number of days with minimum temperature below freezing (Frost) as predictors?


## 3. Regression Method
We will approach this question first by applying stepwise and best subsets regression on the 7 potential predictors to determine the best model. Then we will check LINE conditions on this model using residual analysis. If any of the assumptions are not met, we will transform the data and check LINE conditions for the new model. After fitting the model with transformed data, we will interpret our model and summarize our findings. 

\newpage
## 4. Regression Analysis, Results and Interpretation 

### Variable Selection 
First, we look at the scatterplot matrix to gain some insight on the relationships between the variables in the data. From the scatterplot below, we can tell that there are some predictors like ‘Murder’ seems to be  strongly related to ‘Life Expectancy’. Others like ‘Area’ and ‘Income’ seem to be moderately or weakly related. 
```{r}
pairs(dat[c(4,1,2,3,5,6,7,8)], cex=0.4) #scatterplot matrix
cor(dat)
```

Secondly, we perform variable selection using stepwise regression, including AIC and partial F test, and the best subsets regression to determine the predictors. The results of our AIC test, partial F test, and adjusted R2 criterion chooses four predictors: “Murder”, “HS Grad”, “Frost”, and “Population” . The Mallows’ Cp criterion gives similar result except excluding the fourth predictor “Population”. Therefore, We decide our model to be `Life Exp` ~ Murder + `HS Grad` + Frost + Population.
```{r}
# Stepwise regression using AIC
mod0=lm(`Life Exp`~1)
mod.all = lm(`Life Exp`~., data=dat) # including all predictors in lm()
step(mod0, scope = list(lower = mod0, upper = mod.all))
mod.AIC = lm(`Life Exp` ~ Murder + `HS Grad` + Frost + Population, data=dat)

# Stepwise regression using F-test
mod0=lm(`Life Exp`~1)
add1(mod0, ~.+Population+Income+Illiteracy+Murder+`HS Grad`+Frost+Area, test = 'F')
#choose Murder, which has the smallest p-value or largest F-statistic
mod1 = update(mod0, ~.+Murder)
add1(mod1, ~.+Population+Income+Illiteracy+`HS Grad`+Frost+Area, test = 'F')
#choose HS Grad, which has the smallest p-value or largest F-statistic
mod2 = update(mod1, ~.+`HS Grad`)
#check if Murder is still significant after adding HS Grad
summary(mod2)
#both predictors have very small p-value: significant
add1(mod2, ~.+Population+Income+Illiteracy+Frost+Area, test = 'F')
#choose Frost, which has the smallest p-value or largest F-statistic
mod3 = update(mod2, ~.+Frost)
#check if Murder and HS Grad are still significant after adding Frost
summary(mod3)
#all predictors have very small p-value: significant
add1(mod3, ~.+Population+Income+Illiteracy+Area, test = 'F')
#choose Pop, which has the smallest p-value or largest F-statistic
mod4 = update(mod3, ~.+Population)
#check if Murder, HS Grad, and Frost are still significant after adding Pop
summary(mod4)
#all predictors have very small p-value: significant
add1(mod4, ~.+Income+Illiteracy+Area, test = 'F')
#no more significant predictors, p-values > 0.15
#same model as what we found in AIC

#Best subset regression
library(leaps)
mod = regsubsets(cbind(Population, Income, Illiteracy, Murder, `HS Grad`, Frost, Area), `Life Exp`)
summary.mod = summary(mod)
summary.mod$which
names(summary.mod)
summary.mod$adjr2
# from 3rd to 4th, increased almost 2%
# from 4th to fifith, dropping
# so we choose 4 predictors, look back at matrix, find that same as what we found in stepwise regression
summary.mod$cp
# only C_p close to p is the third one, 3.74 close to p=4, three predictors
```

\newpage
### Diagnostic Checks and Transformation
Thirdly, we check the LINE conditions for this model. We will not be checking the independence assumption, since we are not given data related to time order.
```{r height=4}
# Residuals Analysis
yhat=mod.AIC$fitted.values
e=mod.AIC$residuals
plot(yhat, e, xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Fit')
abline(h = 0, lty = 2)

par(mfrow=c(1,2))
hist(e)
qqnorm(e)
qqline(e)
```
The Residual v.s. Fitted plot shows that residuals “bounce randomly”  and roughly form a “horizontal band” around the y=0 line. However, when looking at the “Residuals vs Predictor” plot, and see a strong funneling effect for the “Residuals v.s. Population ” plot. Since a log function has the ability to “spread out” smaller values and bring in larger ones, we will perform log transformation on “Population”. Our model is now `Life Exp` ~ Murder + `HS Grad` + Frost + log(Population). Then we check our LINE conditions again. 
```{r fig.width=4, fig.height=4}
library(MASS)
boxcox(`Life Exp`~Murder+`HS Grad`+Frost+Population, data=dat)
# choose lambda -1

y <- 1/`Life Exp` #transform y
mod.trans <- lm(y ~ Murder + `HS Grad` + Frost + Population) #fit new model
```

```{r fig.height=4}
#Residuals Analysis again
e2 = resid(mod.trans)
yhat2 = fitted(mod.trans)
plot(yhat, e2, xlab = 'Fitted Values', ylab = 'Residuals' )
abline(h = 0, lty = 2)

par(mfrow=c(1,2))
qqnorm(e2)
qqline(e2)
hist(e2)
```
The “Residuals vs Predictor” plot for log(Population) is well-behaved now. The Residual v.s. Fit plot and Normal Q-Q plot are both well-behaved. There are no unequal variance or nonlinearity problems.


Our final step is checking for outliers and leverage. After computing for both internally studentized residuals and studentized deleted (or externally studentized) residuals, none of them are larger than 3 in absolute value. Thus, there are no unusual Y observations. After computing the hat values, we find that none of the points has higher hat value than 3pn=0.3. Therefore, there are no outliers or leverage points. And we will not need to investigate for any potentially influential points. Our model has met the LINE conditions. 
```{r}
rs=rstandard(mod.trans) # internally studentized residuals
sort(rs)

rsd=rstudent(mod.trans) # studentized deleted
sort(rsd)

n=length(e2)
p=4+1 # four predictors + 1
3*p/n # rules of thumb, 3 times the mean leverage value
hv=hatvalues(mod.trans)
sort(hv)
# 0.385 > .3

2*sqrt((p+1)/(n-p-1))
diff=dffits(mod.trans) # Difference in Fits (DFFITS)
sort(diff)
# no abs val greater than .739

ck=cooks.distance(mod.trans) # Cook's distance measure
sort(ck)
#not influential
```

\newpage
### Interpretation
We are now able to observe our model with 4 predictors: Murder, HS Grad, Frost, log(Population).  \
$Life Expectancy= -0.29Murder+0.0546 HSGrad-0.051 Frost+0.24 log(Population)$
```{r}
summary(mod.trans)
```
From the above summary table of our model, the adjusted R2is 0.7173,  telling us that about 71.73% percent variation in life expectancy is explained by our model. Also, the associated p-value 1.17e-12of the whole model is very small, indicating our model is significant. 

“Murder” has negative coefficients -0.29, meaning that we predict a 1 percent increase in murder rate would result in -0.29 year decrease in the mean life expectancy. Similarly, “Frost” has a coefficient -0.00517, indicating that we expect a 1 unit increase in the mean number of days under freezing would bring 0.00517 year decrease in the mean life expectancy. On the other hand, the positive coefficient of “HS Grad” indicates that 1 percentage increase in high school graduation  increases mean life expectancy by 0.0546 years. And we expect mean life expectancy to increase 0.5684 years for each ten-fold increase in population . (0.56836 = 0.246836 ln(10))

## 5. Conclusion 
In conclusion, we are able to predict the meanlife expectancy of people in a U.S. state given its population, local murder rate, high school graduation percentage, and the mean number of days with minimum temperature below freezing. In general, states with higher population and high school graduation percentage would have longer life expectancy, while higher murder rates and more days in freezing temperatures would result in shorter life expectancy. 

Given that the size of the dataset is limited (including only statistics from each state),  the accuracy could be improved if we are able to draw more data by smaller region, for example, census by county. It would also be helpful if we could draw more possible related predictors into the dataset, for example, the elevation of the region, unemployment rate, healthcare coverage, air quality, etc. We should also note that the data we draw is from the US census in 1977, which means that necessary adjustment is needed with updated data for contemporary prediction. 

