---
title: 'W21 PSTAT 131/231 Final project '
author: "PSTAT131-231"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      eval=T,
                      cache = F,
                      results='markup', 
                      message=F,
                      warning=F, 
                      fig.align = 'center',
                      fig.height = 4, 
                      fig.width = 4)

library(pander)
library(tidyverse)
library(ggmap)
library(modelr)
library(tree)
library(maptree)
library(ROCR)
library(ggridges)
library(dendextend)
library(randomForest)
library(knitr)
```

# Background

1. What makes voter behavior prediction (and thus election forecasting) a hard problem?

Voter behavior prediction is a hard problem for a number of reasons. We begin with what Silver describes as the “nowcast”, a model of how people in the US will vote if the election were held that particular day. As time goes on, voting intention changes due to a number of factors such as race, age, or gender. However, intentions may be changed due to more intangible effects that cannot be measured or observed. This, in turn, can result in misleading polls/predictions that don’t account for the random noise generated by these intangibilities. These polls may over or underestimate the performance of any given candidate due to sampling error or sampling bias. These errors can aggregate from a regional level, to a state level, and finally to the national level to create larger prediction errors.

2. What was unique to Nate Silver's approach in 2012 that allowed him to achieve good predictions?

Silver was able to achieve good predictions for the 2012 election primarily by looking at the full range of probabilities from polls, as opposed to the maximum range of probabilities. This approach allows him to not rely on the “house effect”, where polls arbitrarily correct for bias, instead using his own model to estimate the probability of a shift in voter support for a certain candidate. Silver utilizes hierarchical modelling and his “nowcast” to generate a time series that can be simulated forward to election day, estimating state and national support for each candidate. This is because the information can move around the model, from state to state, day to day, allowing for less biased predictions. Additionally, more polling data came out as the election drew nearer, allowing Silver to constantly update the “nowcast” and reduce the variance in true voting intention, thus allowing his model to make progressively accurate predictions.

3. What went wrong in 2016? What do you think should be done to make future predictions better?

In 2016, the polls wildly mispredicted the results of the election. Individual polls typically have error from statistical noise or nonresponse bias, so the aggregation of these polls on a state level typically reduces the bias and produces more accurate predictions. However, in 2016, polls missed in the same direction, indicating that there was a systematic error, that all of the polls were facing similar challenges no matter their location or method. When these polls were aggregated, particularly in swing states, national polls followed suit and overestimated Clinton’s lead over Trump. This could be seen mainly in the Midwestern states, where Trump over-performed by 4 points or more, winning the majority of the states he was expected to lose. The error in polling prediction is thought to be attributed to the lack of Trump supporters responding to polls or openly expressing their voting intentions. To improve future predictions, anonymity could be implemented into the polls to increase the number of participants from those who are shy or weary of disclosing their voting intentions.

# Data

The `project_data.RData` binary file contains three datasets: tract-level 2010 census data, stored as `census`; metadata `census_meta` with variable descriptions and types; and county-level vote tallies from the 2016 election, stored as `election_raw`.
```{r}
load('data/project_data.RData')
```

## Election data

Some example rows of the election data are shown below:
```{r}
filter(election_raw, !is.na(county)) %>% 
  head() %>% 
  pander()
```

4. Inspect rows with `fips=2000`. Provide a reason for excluding them. Drop these observations -- please write over `election_raw` -- and report the data dimensions after removal. 

```{r}
election_raw %>% 
  filter(fips == 2000) %>% pander(caption = "observations with fips 2000")

election_raw %>% 
  filter(state == "AK") %>% pander(caption = "observations with state AK")

#summary(election_raw) %>% pander(caption = "election_raw before removal summary")
cat("The dimensions before removal are", dim(election_raw)[1], "by", dim(election_raw)[2], ".")

election_old <- election_raw #store old dataset just in case
election_drop <- election_raw %>% filter(fips == 2000) #store just-incase

election_raw <- election_raw %>% 
  filter(fips != 2000)
#summary(election_raw) %>% pander(caption = "election_raw after removal summary")
cat("The dimensions after removal are", dim(election_raw)[1], "by", dim(election_raw)[2], ".")
```
We notice that the election data for the entire state of `AK` and `fips` code 2000 are identical, so we dropped the duplicate observations. Also, `county` values of `NA` should have a `fips` value of `US` or state name. Rows with `fips=2000` have `county` values of `NA`, which contradicts our previous statement. The dimensions before removal are 18351 by 5. After doing so, the new dimensions are 18,345 observations of 5 variables.

## Census data

The first few rows and columns of the `census` data are shown below.
```{r}
census %>% 
  select(1:6) %>% 
  head() %>% 
  pander(digits = 15)
```
Variable descriptions are given in the `metadata` file. The variables shown above are:
```{r}
census_meta %>% head() %>% pander()
```

## Data preprocessing

5. Separate the rows of `election_raw` into separate federal-, state-, and county-level data frames:

    * Store federal-level tallies as `election_federal`.
    
    * Store state-level tallies as `election_state`.
    
    * Store county-level tallies as `election`. Coerce the `fips` variable to numeric.

```{r}
election_federal <- election_raw %>% 
  filter(fips=="US") # federal-level tallies, 32 observations

election_state <- election_raw %>%
  filter(is.na(as.numeric(fips)) & fips!="US") # state-level tallies , 302 observations

election <- election_raw %>%
  mutate(fips = as.numeric(fips)) %>%
  filter(!is.na(fips))   # county-level tallies , 18011 observations
```

6. How many named presidential candidates were there in the 2016 election? Draw a bar graph of all votes received by each candidate, and order the candidate names by decreasing vote counts. (You may need to log-transform the vote axis.)

```{r fig.height=6, fig.width=6}
cat("There were", unique(election_raw$candidate) %>% length(), 
    "named presidential candidates in the 2016 election.")

election_federal %>% 
  ggplot(aes(x = fct_reorder(candidate, votes, .desc=TRUE), 
             y = log(votes))) +
  geom_bar(stat="identity", width=0.7, fill="slateblue3") +
  labs(x = "2016 Presidential Candidates", y = "Log of Total Votes",
       title = "Bar Graph: Log-transform of Total Votes \n Received by Each Candidate") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, vjust=0.3)) 
```

7. Create variables `county_winner` and `state_winner` by taking the candidate with the highest proportion of votes. (Hint: to create `county_winner`, start with `election`, group by `fips`, compute `total` votes, and `pct = votes/total`. Then choose the highest row using `slice_max` (variable `state_winner` is similar).)

```{r}
county_winner <- election %>%
  group_by(fips) %>%
  mutate(total = sum(votes),
         pct = votes/total) %>%
  slice_max(pct)

county_winner %>% head() %>% pander(caption = "county_winner")

state_winner <- election_state %>%
  group_by(state) %>%
  mutate(total = sum(votes),
         pct = votes/total) %>%
  slice_max(pct)

state_winner %>% head() %>% pander(caption = "state_winner")
```

# Visualization

Here you'll generate maps of the election data using `ggmap`. The .Rmd file for this document contains codes to generate the following map.
```{r}
states <- map_data("state")

ggplot(states) + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = region, 
                   group = group), 
               color = "white") + 
  coord_fixed(1.3) + # avoid stretching
  guides(fill=FALSE) + # no fill legend
  theme_nothing() # no axes
```

8. Draw a county-level map with `map_data("county")` and color by county.
```{r}
county <- map_data("county")

ggplot(county) + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = subregion, 
                   group = group), 
               color = "white") + 
  coord_fixed(1.3) + # avoid stretching
  guides(fill=FALSE) + # no fill legend
  theme_nothing() # no axes
```

In order to map the winning candidate for each state, the map data (`states`) must be merged with with the election data (`state_winner`).

The function `left_join()` will do the trick, but needs to join the data frames on a variable with values that match. In this case, that variable is the state name, but abbreviations are used in one data frame and the full name is used in the other.

9. Use the following function to create a `fips` variable in the `states` data frame with values that match the `fips` variable in `election_federal`.
```{r}
name2abb <- function(statename){
  ix <- match(statename, tolower(state.name))
  out <- state.abb[ix]
  return(out)
}

states$fips <- name2abb(states$region)

states %>% head() %>% pander(caption = "states$fips")
```

Now the data frames can be merged. `left_join(df1, df2)` takes all the rows from `df1` and looks for matches in `df2`. For each match, `left_join()` appends the data from the second table to the matching row in the first; if no matching value is found, it adds missing values.

10. Use `left_join` to merge the tables and use the result to create a map of the election results by state. Your figure will look similar to this state level [New York Times map](https://www.nytimes.com/elections/results/president). (Hint: use `scale_fill_brewer(palette="Set1")` for a red-and-blue map.)

```{r}
results_state <- left_join(states, state_winner)

ggplot(results_state) + 
  geom_polygon(aes(x = long, 
                   y = lat,
                   fill = candidate, # election results
                   group = group), 
               color = "white") + 
  scale_fill_brewer(palette = "Set1") + # for a red-and-blue map
  coord_fixed(1.3) + # avoid stretching
  guides(fill=FALSE) + # no fill legend
  theme_nothing() # no axes
```

11. Now create a county-level map. The county-level map data does not have a `fips` value, so to create one, use information from `maps::county.fips`: split the `polyname` column to `region` and `subregion` using `tidyr::separate`, and use `left_join()` to combine `county.fips` with the county-level map data. Then construct the map. Your figure will look similar to county-level [New York Times map](https://www.nytimes.com/elections/results/president).

```{r}
county.fips <- maps::county.fips %>% 
  separate(col = polyname, into = c("region", "subregion"), sep = ",")
results_county <- left_join(county.fips, county_winner)
results_county <- left_join(county, results_county)

ggplot(results_county) + 
  geom_polygon(aes(x = long, 
                   y = lat,
                   fill = candidate, # election results
                   group = group), 
               color = "white") + 
  scale_fill_brewer(palette = "Set1") + # for a red-and-blue map
  coord_fixed(1.3) + # avoid stretching
  guides(fill=FALSE) + # no fill legend
  theme_nothing() # no axes
```

12. Create a visualization of your choice using `census` data. Many exit polls noted that [demographics played a big role in the election](https://fivethirtyeight.com/features/demographics-not-hacking-explain-the-election-results/). If you need a starting point, use [this Washington Post article](https://www.washingtonpost.com/graphics/politics/2016-election/exit-polls/) and [this R graph gallery](https://www.r-graph-gallery.com/) for ideas and inspiration.
```{r fig.width=6}
minority_income <- census %>% 
  na.omit() %>% 
  mutate(Minority = Hispanic + Black + Native + Asian + Pacific) %>% 
  select(-c(Hispanic, Black, Native, Asian, Pacific)) %>% 
  select(c(State, Minority, Income)) %>% 
  group_by(State) %>% 
  arrange(Income) %>% 
  summarise(across(c(1,2), mean)) %>% 
  filter(State != "Puerto Rico")
fips <- c("AL", "AK", "AZ", "AR","CA", "CO", "CT", "DE", "DC", "FL", "GA", "HI", "ID", "IL", "IN", "IA", 
          "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", 
          "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", 
          "WV", "WI", "WY")
minority_income$fips <- fips
demographic <- left_join(minority_income, state_winner[,2:3])

ggplot(demographic, aes(x = Income, y = Minority)) +
  geom_point(aes(color = candidate)) +
  geom_text(aes(label = fips, hjust = 0.5, vjust = -0.4), size=3.5) +
  labs(y = "Minority (%)", title = "Average Household Income in \n Relation to State Minority %") +
  theme_bw()
```

13. The `census` data contains high resolution information (more fine-grained than county-level). Aggregate the information into county-level data by computing population-weighted averages of each attribute for each county by carrying out the following steps:
    
* Clean census data, saving the result as `census_del`: 
  
   + filter out any rows of `census` with missing values;
   + convert `Men`, `Employed`, and `Citizen` to percentages;
   + compute a `Minority` variable by combining `Hispanic`, `Black`, `Native`, `Asian`, `Pacific`, and remove these variables after creating `Minority`; and
   + remove `Walk`, `PublicWork`, and `Construction`.
 
* Create population weights for sub-county census data, saving the result as `census_subct`: 
    + group `census_del` by `State` and `County`;
    + use `add_tally()` to compute `CountyPop`; 
    + compute the population weight as `TotalPop/CountyTotal`;
    + adjust all quantitative variables by multiplying by the population weights.
    
* Aggregate census data to county level, `census_ct`: group the sub-county data `census_subct` by state and county and compute popluation-weighted averages of each variable by taking the sum (since the variables were already transformed by the population weights)
    
* Print the first few rows and columns of `census_ct`. 
```{r}
# Clean census data
census_del <- census %>% 
  drop_na() %>% # filter out any rows of with missing values;
  mutate(Men=Men/TotalPop*100,
         Employed=Employed/TotalPop*100,
         Citizen=Citizen/TotalPop*100)  %>%    # convert to percentages
  mutate(Minority=Hispanic+Black+Native+Asian+Pacific) %>% # compute a Minority variable by combining
  select(-c(Hispanic,Black,Native,Asian,Pacific,Walk,PublicWork,Construction)) # remove, remove

# Create population weights
census_subct <- census_del %>% 
  group_by(State, County) %>% 
  add_tally(TotalPop, name="CountyPop") %>%
  mutate(pop_wt=TotalPop/CountyPop) %>%
  mutate(across(c(Men:Minority), ~ .x *pop_wt)) # adjust all quant vars by multiplying

census_ct <- census_subct %>%
  group_by(State, County) %>% 
  summarise(across(c(TotalPop:Minority), sum)) 
  # Censustract goes away, CountyPop goes away (now totalpop), pop_wt kept to check if 1

census_ct %>% head() %>% select(1:8) %>% 
  pander(caption = "the first few rows and columns of `census_ct`") # Print the first few 
```

14. If you were physically located in the United States on election day for the 2016 presidential election, what state and county were you in? Compare and contrast the results and demographic information for this county with the state it is located in. If you were not in the United States on election day, select any county. Do you find anything unusual or surprising? If so, explain; if not, explain why not.
```{r}
county_winner %>% filter (county == "Santa Clara County") %>% 
  pander(caption = "Voting results in Santa Clara County")
census_ct %>% filter (County == "Santa Clara") %>% 
  pander(caption = "Demographic information of Santa Clara County")
```
On election day for the 2016 presidential election, one of us was physically located in Santa Clara County, California. Hillary Clinton won with 73.39% of the vote. I did not find anything unusual or surprising. This is because Santa Clara county is a relatively progressive area in a typically blue state. The key demographic that makes this result unsurprising is the diversity in the area, with the minority population being 63.06% and the white population being 33.58%. We saw in #12 that states with higher minority populations tended to vote for Clinton, so this is consistent with those results.

# Exploratory analysis

15. Carry out PCA for both county & sub-county level census data. Compute the first two principal components PC1 and PC2 for both county and sub-county respectively. Discuss whether you chose to center and scale the features and the reasons for your choice. Examine and interpret the loadings.
```{r}
# for county
# extract features and center and scale (30-2,-1-1)
x_mx <- census_ct %>% 
  ungroup() %>% 
  select(-c(State, County, Women)) %>% 
  scale(center = T, scale = T)

x_svd <- svd(x_mx)  # compute SVD

v_svd <- x_svd$v  # get loadings

z_mx <- x_mx %*% x_svd$v  # compute PCs

# examine the data plotted on the first two PCs
z_mx[, 1:2] %>%
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2) %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point(alpha = 0.2, color = "red") +
  ggtitle("PC1 V.S. PC2 for county level") +
  theme_bw()
```

```{r  fig.width = 8}
## plot loadings
v_svd[, 1:2] %>%
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2) %>%
  mutate(variable = colnames(x_mx)) %>%
  gather(key = 'PC', value = 'Loading', 1:2) %>%
  arrange(variable) %>%
  ggplot(aes(x = variable, y = Loading)) +
  geom_point(aes(shape = PC)) +
  theme_bw() +
  geom_hline(yintercept = 0, color = 'blue') +
  geom_path(aes(linetype = PC, group = PC, col=PC)) +
  theme(axis.text.x = element_text(angle = 90)) +
  ggtitle("PC Loadings at County level") +
  labs(x = '')
```

```{r}
# for sub-county
# extract features and center and scale (32-3,-1,-1)
x_mxs <- census_subct %>%
  ungroup() %>%
  select(-c(State, County, CensusTract, Women, pop_wt)) %>%
  scale(center = T, scale = T)

x_svds <- svd(x_mxs)  # compute SVD

v_svds <- x_svds$v  # get loadings

z_mxs <- x_mxs %*% x_svds$v  # compute PCs

# examine the data plotted on the first two PCs
z_mxs[, 1:2] %>%
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2) %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point(alpha = 0.2, color = "purple") +
  ggtitle("PC1 vs PC2 for sub-county level") +
  theme_bw()
```

```{r fig.width = 8}
## plot loadings
v_svds[, 1:2] %>%
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2) %>%
  mutate(variable = colnames(x_mxs)) %>%
  gather(key = 'PC', value = 'Loading', 1:2) %>%
  arrange(variable) %>%
  ggplot(aes(x = variable, y = Loading)) +
  geom_point(aes(shape = PC)) +
  theme_bw() +
  geom_hline(yintercept = 0, color = 'blue') +
  geom_path(aes(linetype = PC, group = PC, col=PC)) +
  theme(axis.text.x = element_text(angle = 90)) +
  ggtitle("PC Loadings at Sub-county level") +
  labs(x = '')
```
We chose to center and scale the features because the variables have different units and scales. PCA with unscaled variables upweights variables with the highest variances. So scaling is a good idea as a rule of thumb when doing PCA analysis.

County: \
**PC1** will be large whenever the employed, income, incomepercap, incomepercaperr, professional, white variables are low and the variables -- childpoverty, minority, poverty, and unemployment-- are high. This seems to describe counties with poor economic infrastructure, thus having high unemployment and poverty rates made up primarily of minority populations. \
**PC2** will be large whenever the citizen, familywork, self-employed, and white variables are low and the variables -- income, incomeerr, incomepercaperr, minority, office, totalpop, and transit variables-- are high. This seems to describe counties with large, high density cities and few suburbs. \

Sub-county: \
**PC1** will be large whenever all variables except countypop and totalpop are relatively high. This seems to describe subcounties with small towns and moderate diversity, likely composed of suburban or more remote areas. \
**PC2** will be large whenever the childpoverty, minority, poverty, and unemployment variables are low and the variables --familywork, selfemployed, white, and workathome-- are high. This seems to describe subcounties with low diversity (mostly white) and small business, family-oriented areas. \


16. Determine the minimum number of PCs needed to capture 90% of the variance for both the county and sub-county analyses. Plot the proportion of variance explained and cumulative variance explained for both county and sub-county analyses.
```{r, fig.width = 8}
# for county
pc_vars <- x_svd$d^2/(nrow(x_mx) - 1)  # compute PC variances

# scree and cumulative variance plots
tibble(PC = 1:min(dim(x_mx)),
       Proportion = pc_vars/sum(pc_vars),
       Cumulative = cumsum(Proportion)) %>%
  gather(key = 'measure', value = 'Variance Explained', 2:3) %>%
  ggplot(aes(x = PC, y = `Variance Explained`)) +
  geom_point() +
  geom_path() +
  facet_wrap(~ measure) +
  theme_bw() +
  scale_x_continuous(breaks = 1:30, labels = as.character(1:30)) +
  scale_y_continuous(n.breaks = 10) +
  geom_vline(xintercept = 13,
             color = 'red',
             linetype = 2) +
  ggtitle("Proportion and Cumulative of Variance Plots for county level")
```
The minimum number of PCs needed to capture 90% of the variance for the county analysis is **13**.

```{r, fig.width = 8}
# for sub-county
# compute PC variances
pc_varss <- x_svds$d^2/(nrow(x_mxs) - 1)  # compute PC variances

# scree and cumulative variance plots
tibble(PC = 1:min(dim(x_mxs)),
       Proportion = pc_varss/sum(pc_varss),
       Cumulative = cumsum(Proportion)) %>%
  gather(key = 'measure', value = 'Variance Explained', 2:3) %>%
  ggplot(aes(x = PC, y = `Variance Explained`)) +
  geom_point(size=1) +
  geom_path() +
  facet_wrap(~ measure) +
  theme_bw() +
  scale_x_continuous(breaks = 1:30, labels = as.character(1:30)) +
  scale_y_continuous(n.breaks = 10) +
  geom_vline(xintercept = 7,
             color = 'red',
             linetype = 2) +
  ggtitle("Proportion and Cumulative of Variance Plots for sub-county level")
```
The minimum number of PCs needed to capture 90% of the variance for the sub-county analysis is **7**. \

17. With `census_ct`, perform hierarchical clustering with complete linkage.  Cut the tree to partition the observations into 10 clusters. Re-run the hierarchical clustering algorithm using the first 5 principal components the county-level data as inputs instead of the original features. Compare and contrast the results. For both approaches investigate the cluster that contains San Mateo County. Which approach seemed to put San Mateo County in a more appropriate cluster? Comment on what you observe and discuss possible explanations for these observations.
```{r}
set.seed(12521)
# center and scale data
brfss_std <- census_ct %>% 
  ungroup() %>% 
  select(-c(State, County, Women)) %>% 
  scale(center = T, scale = T) %>% 
  as.data.frame()

# perform hierarchical clustering with complete linkage
d_mx <- dist(brfss_std, method = 'euclidean') # compute distances between points
hclust_out <- hclust(d_mx, method = 'complete') # compute hierarchical clustering

# Cut the tree to partition the observations into 10 clusters
clusters <- cutree(hclust_out, k = 10) %>%
  factor(labels = paste('cluster', 1:10))
#tibble(clusters) %>% count(clusters) %>% pander(caption = "clusters count w/ the original features")

# plot clusters
brfss_std %>%
  mutate(cluster = clusters) %>%
  gather(key = 'variable', value = 'value', 1:dim(brfss_std)[2]) %>%
  ggplot(aes(y = variable, x = value)) + 
  geom_density_ridges(aes(fill = cluster), 
                      bandwidth = 0.2,
                      alpha = 0.5) + 
  theme_minimal() + 
  xlim(c(-4, 4)) + 
  labs(y = '', title='ridge plot of clusters') 

## Re-run using the first 5 principal components the county-level data

# get loadings for first 5 pcs
pc_loadings <- x_svd$v[, 1:5]
# compute principal components
census_ct_pc <- data.frame(as.matrix(brfss_std) %*% pc_loadings)
colnames(census_ct_pc) <- paste('PC', 1:5, sep = '')

# perform hierarchical clustering with complete linkage
d_mx_pc <- dist(census_ct_pc, method = 'euclidean') # compute distances between points
hclust_out_pc <- hclust(d_mx_pc, method = 'complete') # compute hierarchical clustering

# Cut the tree to partition the observations into 10 clusters
clusters_pc <- cutree(hclust_out_pc, k = 10) %>%
  factor(labels = paste('cluster', 1:10))

# plot clusters
census_ct_pc %>%
  mutate(cluster = clusters_pc) %>%
  gather(key = 'variable', value = 'value', 1:5) %>%
  ggplot(aes(y = variable, x = value)) + 
  geom_density_ridges(aes(fill = cluster), 
                      bandwidth = 0.2,
                      alpha = 0.5) + 
  theme_minimal() + 
  xlim(c(-8, 8)) + 
  labs(y = '', title='ridge plot using pcs')

# Compare and contrast the results
tibble(clusters) %>% count(clusters) %>%
  bind_cols( tibble(clusters_pc) %>% count(clusters_pc) ,
             .name_repair = c("minimal")) %>% pander(caption="count number of obs per cluster")
```

```{r}
# investigate the cluster that contains San Mateo County
clus_census<- brfss_std %>%
  mutate(cluster = clusters) %>%
  mutate(County = census_ct$County)

SM_clus <- clus_census %>% filter (County == "San Mateo") %>% pull(cluster)

#calc summ stats
SM_clus_census_ct <- clus_census %>%      #dataw/org features, clus, and county names
  filter(cluster==SM_clus) %>%                #choose clusthat SM is in
  summarise(across(c(TotalPop:Minority), mean)) %>%  
  mutate(cluster=SM_clus)  

# PCs, investigate the cluster that contains San Mateo County
pc_clus_census <- census_ct_pc %>%
  mutate(cluster = clusters_pc) %>%
  mutate(County = census_ct$County)

SM_pc_clus <- pc_clus_census %>% filter (County == "San Mateo") %>% pull(cluster)

#calc summ stats
SM_pc_clus_census_ct <- clus_census %>%  #dataw/org features, org clus, and county names
  mutate(cluster=clusters_pc) %>%        # change cluster to pc's
  filter(cluster==SM_pc_clus) %>%                #choose pcclusthat SM is in
  summarise(across(c(TotalPop:Minority), mean)) %>%  
  mutate(cluster=SM_pc_clus)   

# get SM vars
SM_census_ct <- clus_census %>% 
  filter (County == "San Mateo") %>% select(-cluster)  

# calculate ecludeian distance

num<-dim(brfss_std)[2]
SM_dist <- dist(rbind(SM_clus_census_ct[1:num], 
                      SM_census_ct[1:num]), 
                method = "euclidean")
SM_pc_dist <- dist(rbind(SM_pc_clus_census_ct[1:num], 
                          SM_census_ct[1:num]), 
                    method = "euclidean")

SM_compare <- SM_clus_census_ct %>% bind_rows(SM_pc_clus_census_ct, SM_census_ct) 
SM_compare <- as.data.frame(SM_compare) %>% mutate(dist=c(SM_dist, SM_pc_dist, NA) ) 
SM_compare %>% t %>% pander(caption="compare variables: original features vs PCs vs SanMateo")

# plot cluster with SM
SM_pt <- SM_census_ct %>% select(1:dim(brfss_std)[2]) %>% as.data.frame()
SM_pt <- SM_pt %>% 
  gather(key = 'variable', value = 'value')

brfss_std %>%
  mutate(cluster = clusters) %>%
  filter(cluster==SM_clus) %>%
  gather(key = 'variable', value = 'value', 1:dim(brfss_std)[2]) %>%
  ggplot(aes(y = variable, x = value)) + 
  geom_density_ridges(aes(fill = cluster), 
                      bandwidth = 0.2,
                      alpha = 0.5) + 
  geom_point(color = "blue", size = 1, data=SM_pt) +
  theme_minimal() + 
  xlim(c(-4, 4)) + 
  labs(y = '') 

SM_pc_pt <- pc_clus_census %>% 
  filter (County == "San Mateo") %>% select(1:dim(census_ct_pc)[2]) %>% as.data.frame()
SM_pc_pt <- SM_pc_pt %>% 
  gather(key = 'variable', value = 'value')

census_ct_pc %>%
  mutate(cluster = clusters_pc) %>%
  filter(cluster==SM_pc_clus) %>%
  gather(key = 'variable', value = 'value', 1:5) %>%
  ggplot(aes(y = variable, x = value)) + 
  geom_density_ridges(aes(fill = cluster), 
                      bandwidth = 0.2,
                      alpha = 0.5) + 
  geom_point(color = "blue", size = 1, data=SM_pc_pt) +
  theme_minimal() + 
  xlim(c(-8, 8)) + 
  labs(y = '') 
```

As seen in the table above, the distribution of observations using the first 5 pcs, is more evenly spread out. Looking at the table above (with the number of observations in each cluster), we see that the data when clustering with the original features is highly concentrated into cluster 1, so it is unlikely that this will yield useful classifications. The reduced dimensionality of the first 5 principal components allows for more accurate clustering, which is exemplified in our output for San Mateo County.

The second approach, using the first 5 principal components instead of the original features, seemed to put San Mateo County in a more appropriate cluster. The coefficients for each variable using the PC’s are more similar to the true coefficients than those produced by the original features. 

Further, we calculated the euclidean distance between the mean of the cluster that San Mateo belongs to and the features of San Mateo itself. We observed that San Mateo’s features (blue points) are closer to the center of its cluster when using the first 5 PC’s (see second ridgeplot). This confirms our conclusion that the PC approach classifies counties in a more appropriate cluster. This is probably because, when we use PC’s, the lower dimensionality improves the distance models’ prediction accuracy. 




\newpage
# Classification

In order to train classification models, we need to combine `county_winner` and `census_ct` data. This seemingly straightforward task is harder than it sounds. Codes are provided in the .Rmd file that make the necessary changes to merge them into `election_cl` for classification.
```{r}
abb2name <- function(stateabb){
  ix <- match(stateabb, state.abb)
  out <- tolower(state.name[ix])
  return(out)
}

tmpwinner <- county_winner %>%
  ungroup %>%
  # coerce names to abbreviations
  mutate(state = abb2name(state)) %>%
  # everything lower case
  mutate(across(c(state, county), tolower)) %>%
  # remove county suffixes
  mutate(county = gsub(" county| columbia| city| parish", 
                       "", 
                       county)) 

tmpcensus <- census_ct %>% 
  ungroup() %>% # added by us
  mutate(across(c(State, County), tolower))

election_county <- tmpwinner %>%
  left_join(tmpcensus, 
            by = c("state"="State", "county"="County")) %>% 
  na.omit()

## save meta information
election_meta <- election_county %>% 
  select(c(county, fips, state, votes, pct, total))

## save predictors and class labels
election_county <- election_county %>% 
  select(-c(county, fips, state, votes, pct, total))
```
After merging the data, partition the result into 80% training and 20% testing partitions.

18. Decision tree: train a decision tree on the training partition, and apply cost-complexity pruning. Visualize the tree before and after pruning. Estimate the misclassification errors on the test partition, and intepret and discuss the results of the decision tree analysis. Use your plot to tell a story about voting behavior in the US (see this [NYT infographic](https://archive.nytimes.com/www.nytimes.com/imagepages/2008/04/16/us/20080416_OBAMA_GRAPHIC.html)).
```{r, fig.height = 8, fig.width = 12}
election_county$candidate <- factor(election_county$candidate)

# Divide the whole set into training and test sets:
set.seed(12521) 
election_county_part <- resample_partition(election_county, c(test = 0.2, train = 0.8))
train <- as_tibble(election_county_part$train)
test <- as_tibble(election_county_part$test)

# Make a complex tree:
nmin <- 5
tree_opts <- tree.control(nobs = nrow(train),
                          minsize = nmin,
                          mindev = exp(-8))
t_0 <- tree(candidate ~ ., data = train,
            control = tree_opts, split = 'deviance')
draw.tree(t_0, cex = 0.3, digits = 2)
# count leaf nodes and splits on each variable
#t_0$frame %>%
#  count(var) %>%
#  arrange(desc(n)) %>%
#  pander()

# Prune the tree:
nfolds <- 8
cv_out <- cv.tree(t_0, K = nfolds)
cv_df <- tibble(alpha = cv_out$k,
                impurity = cv_out$dev,
                size = cv_out$size) # convert to tibble
best_alpha <- slice_min(cv_df, impurity) %>%
  slice_min(size)  # select largest alpha with minimum impurity
#best_alpha %>% pander()

# Draw a graph:
#cv_df %>%
#  ggplot(aes(x=alpha, y=impurity)) +
#  geom_point(size=1) +
#  geom_line(color="blue", size=0.8) +
#  geom_point(data=best_alpha, color="red", size=1.8) +
#  theme_bw()

# extract corresponding tree
t_opt <- prune.tree(t_0, k = best_alpha$alpha)
draw.tree(t_opt, cex = 0.8, digits = 2) 
t_opt$frame %>%
  count(var) %>%
  arrange(desc(n)) %>%
  pander(caption="examine decision tree nodes") 

# Compute misclassification errors on the test set:
candidate.pred.test <- predict(t_opt, newdata=test, type="class")
errors <- table(y=test$candidate, y_hat=candidate.pred.test)
errors %>% pander(caption="misclassification errors")
(errors/rowSums(errors)) %>% pander(caption="class-wise error rates")
cat("the misclassification error rate is ", mean(candidate.pred.test != test$candidate))
```
The decision tree after pruning returns a misclassification rate of 0.0995 with 14 terminal nodes, split primarily on variables White, Men, and TotalPop. We see from our pruned tree that the first variable the data is split on is White, implying that counties with lower white populations are more likely to vote for Clinton. The next variable the tree splits on is Transit, where counties that utilize public transit tend towards Trump, so we suspect that this may correlate with county demographics and wealth distribution. Thus, we see that counties with higher transit rates and total populations, which are likelier to be lower income counties, usually vote for Clinton. Conversely, counties with lower, white populations are more likely to vote for Trump.

19. Train a logistic regression model on the training partition to predict the winning candidate in each county and estimate errors on the test partition. What are the significant variables? Are these consistent with what you observed in the decision tree analysis? Interpret the meaning of one or two significant coefficients of your choice in terms of a unit change in the variables. Did the results in your particular county (from question 14) match the predicted results?  
```{r}
# fit logistic regression linear in x1 and x2
fit_glm <- glm(candidate ~ ., family = 'binomial', data = train)
#fit_glm
preds_glm <- predict(fit_glm, test, type = 'response') # compute estimated probabilities
y_hat_test_glm <- factor(preds_glm > 0.5, labels = levels(test$candidate)) # bayes classifier

# errors
error_glm <- table(y=test$candidate, y_hat_test_glm)
error_glm %>% pander(caption="misclassification errors")
(error_glm/rowSums(error_glm)) %>% pander(caption="class-wise error rates")
cat("the misclassification error rate is ", mean(test$candidate != y_hat_test_glm, na.rm=TRUE) )

# signifi
summary(fit_glm)$coefficients %>% kable(digits = 5, scientific = FALSE, caption="Summary table")
#summary(fit_glm)$coefficients %>% pander()
#compare with dectreeanalysis

#  results in your particular county from q14
sc_row <- which(election_meta$county=="santa clara")
preds_all <- predict(fit_glm, election_county, type = 'response') 
y_hat_glm <- factor(preds_all > 0.5, labels = levels(test$candidate))
bind_cols(result=election_county$candidate[sc_row],
          predict=y_hat_glm[sc_row]) %>% pander(caption="results in our particular county from q14")
```



The significant variables are White, Citizen, Income, IncomePerCap, Professional, Service, Office, Production, Drive , Carpool, MeanCommute, Employed, PrivateWork, FamilyWork, and Unemployment.
These are _not_ entirely consistent with what we observed in the decision tree analysis (Table 19). \
Holding all other variables constant, a one percent increase in Employed(16+) in the county, on average, increases the odds of this county voting for "Hillary Clinton" by a factor of $e^0.19553$ \
The results in our particular county (from question 14) **did** match the predicted results, as Clinton was predicted to win in Santa Clara County and she did, in fact, win in the election.


20.  Compute ROC curves for the decision tree and logistic regression using predictions on the test data, and display them on the same plot. Based on your classification results, discuss the pros and cons of each method. Are the different classifiers more appropriate for answering different kinds of questions about the election?
```{r fig.height=3, fig.width=5}
# DECISION TREE first
# compute predictions
preds <- predict(t_opt, test)
class <- as.data.frame(test) %>% pull(candidate)
topt_pred <- prediction(predictions = preds[, 2],
                        labels = class)
topt_perf <- performance(topt_pred, 'tpr', 'fpr')

# calculate fpr and tpr
rates_df <- tibble(tpr = slot(topt_perf, 'y.values'),
                  fpr = slot(topt_perf, 'x.values'),
                  alpha = slot(topt_perf, 'alpha.values')) %>%
  unnest(everything()) %>%
  mutate(youden = tpr - fpr)

# select best threshold
opt_thresh <- slice_max(rates_df, youden)
#opt_thresh %>% pander() # print best threshold

# for Logistic Reg
pred_logit <- prediction(predictions = preds_glm, labels = test$candidate) # create prediction object
perf_logit <- performance(pred_logit, 'tpr', 'fpr') # compute error rates as a function of probability threshhold
rates_log <- tibble(tpr = slot(perf_logit, 'y.values'),
                   fpr = slot(perf_logit, 'x.values'),
                   alpha = slot(perf_logit, 'alpha.values')) %>%
  unnest(everything()) %>%
  mutate(youden = tpr - fpr)

# select optimal threshold
best_thresh <- slice_max(rates_log, youden)
#best_thresh %>% pander()

#  display them on the same plot
tibble() %>%
  ggplot(aes(x = fpr, y = tpr)) +
  
  geom_line( aes(color="Dec Tree"), alpha=0.5, lwd=1.2,
            data=as_tibble(rates_df)) +
  geom_point( color="red", data=opt_thresh) +
  
  geom_line( aes(color="Log Reg"), size=0.8, 
             data=as_tibble(rates_log)) +
  geom_point( color="red", data=best_thresh) +
  
  theme_bw() +
  labs(x = 'False positive rate',
       y = 'True positive rate',
       title = 'Decision Tree and Logistic regression ROC') +
  scale_colour_manual(name="method",
                      values=c("Dec Tree"="blue","Log Reg"="black"))
```
Based on our classification results, the misclassification error rate of the decision tree is higher than that of the logistic regression. The graph of the two ROC curves confirms this, as the area under the logistic regression’s curve (black) is greater than that of the decision tree(blue), indicating a higher true positive rate. \
Results of a decision tree analysis are easy to interpret and explain. As seen in our answer for Question 18, we can easily tell an understandable story about the voting behavior by looking at our treemap. A con of decision trees is that, though they are usually interpretable, they can be complex with large variation, which is not optimal when predicting only 2 unique outcomes. \
Logistic regression works (better) in high dimensions and with noise variables. In this case, the census data we are using here has a lot of variables and possible noise variables. A con of logistic regression is that if a decision is split on a nonlinear variable, the subsequent decision boundary will be nonlinear and thus have poorer classification rates. \
Different classifiers are more appropriate for answering different kinds of questions about the election. The decision tree is more appropriate when we want to interpret voting trends in counties and sub-counties or deduce the unique characteristics of these areas, while the logistic regression works better when we want to predict only the final result or winning candidate in a county.




\newpage
# Taking it further

21. \

```{r}
# try random forest
set.seed(12521)
fit_rf <- randomForest(candidate ~
                         ., ntree = 500, mtry = 6, data = train, importance = T)
#fit_rf

fit_rf$importance %>%
  as_tibble() %>%
  select(MeanDecreaseAccuracy) %>%
  mutate(var = factor(rownames(fit_rf$importance))) %>%
  arrange(desc(MeanDecreaseAccuracy)) %>% head(10) %>% pander()

rf_preds <- predict(fit_rf, test)
# compute classification rates
errors_rf <- table(y=test$candidate, y_hat=rf_preds)
errors_rf %>% pander(caption="misclassification errors")
(errors_rf/rowSums(errors_rf)) %>% pander(caption="class-wise error rates")
cat("the misclassification error rate is ", mean(as.numeric(test$candidate) != as.numeric(rf_preds)) )
```

We explored the random forest method on the same training partition using 500 trees and 6 mtry, and received a lower misclassification error than question 18 and 19. This is because a random forest aggregates many decision trees to limit overfitting as well as error due to bias. Therefore, yields better results. However, it is hard to interpret the results of random forest. We can no longer plot the tree and observe which variables were used in splits, although we are able to measure variable importance and can see that the variables White, Minority, and TotalPop appear to be the most important, based on mean accuracy decrease.

Through this analysis, we see that voter prediction behavior, in the case of the 2016 election, is difficult to quantify. In the case of linear regression and decision trees, the misclassification error is relatively high, consistent with the nationwide mispredictions of the election results from polls. The 2016 election was unique in that the polls were collectively misguided by voting intention, so naturally prediction overestimated Hillary Clinton’s performance. Ultimately, we select random forest as our best predictive model, as it has the lowest misclassification error rate among our tested methods. This is likely due to the fact that random forests use bagging to reduce correlation among trees, thereby reducing prediction variance and increasing the performance of our model.


Our main takeaway from this project is that prediction is difficult to quantify due to myriad random variables, tangible and intangible. Especially in the case of large datasets like the ones we used for the 2016 election, analysis of each variable and how it relates to others is paramount to predictive success. When fitting predictive models, we must always account for bias, whilst also checking the performance of the models themselves, either through misclassification rates or visualizing the data with useful plots.


```{r}
# additional visuals
t <- 0.52
cols <- c("Td" = "red3", "Tl" = "red", "Cd" = "blue3", "Cl" = "blue")
state_w <- state_winner %>% 
  mutate(pct_col = case_when(candidate == state_winner$candidate[1] & pct>t ~ 'Td',
                             candidate == state_winner$candidate[1] & pct<=t ~ 'Tl',
                             candidate != state_winner$candidate[1] & pct>t ~ 'Cd',
                             candidate != state_winner$candidate[1] & pct<=t ~ 'Cl'))
res_state <- left_join(states, state_w) 
ggplot(res_state) + 
  geom_polygon(aes(x = long, 
                   y = lat,
                   fill = pct_col, # election results
                   group = group), 
               color = "white") + 
  scale_fill_manual(values = cols) + # for a red-and-blue map
  coord_fixed(1.3) + # avoid stretching
  guides(fill=FALSE) + # no fill legend
  theme_nothing() # no axes

# counties barely win
county_w <- county_winner %>% 
  mutate(pct_col = case_when(candidate == state_winner$candidate[1] & pct>t ~ 'Td',
                           candidate == state_winner$candidate[1] & pct<=t ~ 'Tl',
                           candidate != state_winner$candidate[1] & pct>t ~ 'Cd',
                           candidate != state_winner$candidate[1] & pct<=t ~ 'Cl'))

res_county <- left_join(county.fips, county_w)
res_county <- left_join(county, res_county)

ggplot(res_county) + 
  geom_polygon(aes(x = long, 
                   y = lat,
                   fill = pct_col, # election results
                   group = group), 
               color = "white") + 
  scale_fill_manual(values = cols) + # for a red-and-blue map
  coord_fixed(1.3) + # avoid stretching
  guides(fill=FALSE) + # no fill legend
  theme_nothing() # no axes
```

```{r, fig.width=6}
color_fun <- function(diff, candidate) {
  if (abs(diff) < 0.01) {
    color = "Purple"
    } else if (candidate == "Hillary Clinton") {
      color = "Blue"
      } else {
        color = "Red"
        }
  color
}

wcolors <- election %>% 
  group_by(fips) %>% 
  mutate(total = sum(votes)) %>% 
  mutate(first = max(votes)) %>% 
  mutate(second = nth(votes, 2)) %>% 
  mutate(d = (first - second)/total) %>% 
  slice_max(votes) %>% 
  mutate(colors = color_fun(d, candidate)) %>% 
  mutate(colors = as.factor(colors))

county_colors <- left_join(results_county, wcolors)

purplec <- county_colors %>% filter(colors == "Purple") 
cat("There were", length(unique(purplec$county)), "purple counties.")

ggplot(county_colors) +
  geom_polygon(aes(x = long,
                   y = lat,
                   fill = colors,
                   group = group),
               color = "white") +
  coord_fixed(1.3) + # avoid stretching
  guides(fill=FALSE) + # no fill legend
  scale_fill_manual(values = c("red", "blue", "darkorchid2")) +
  theme_nothing() # no axes
```


```{r fig.height=20, fig.width=10}
tmpcolors <- wcolors %>%
  ungroup %>%
  # coerce names to abbreviations
  mutate(state = abb2name(state)) %>%
  # everything lower case
  mutate(across(c(state, county), tolower)) %>%
  # remove county suffixes
  mutate(county = gsub(" county| columbia| city| parish", 
                       "", 
                       county))

election_colors <- tmpcolors %>%
  left_join(tmpcensus, 
            by = c("state"="State", "county"="County")) %>% 
  na.omit()

colors_county <- election_colors %>% 
  select(-c(county, fips, candidate, state, votes, 
            total, first, second, d, Women))

colors_county %>%
  na.omit() %>% 
  mutate_if(is.numeric, ~scale(., center = T, scale = T)) %>% 
  gather(key = 'variable', value = 'value', 2:length(colors_county)) %>%
  ggplot(aes(y = variable, x = value)) +
  geom_density_ridges(aes(fill = colors), 
                      bandwidth = 0.2,
                      alpha = 0.3,
                      scale=2) +
  theme_minimal() +
  scale_fill_manual(name="county", values = c("red", "blue", "green3")) +
  xlim(c(-4, 4)) +
  labs(y = '') 
```

We are interested in looking at the purple counties – the counties which Clinton and Trump received roughly equal votes. Based on our ridge plot, we see that these counties are nearly identical to their blue and red counterparts.  This makes prediction difficult since there are few discerning variables between what makes a county lean blue or red. The variables that appear to distinguish purple counties the most are White, Minority, and Income. Counties with slightly higher minority populations, and thus slight lower white populations, with lower household incomes are the characteristic of a purple county. However, these differences are not typically significant enough to make an accurate prediction about how they’ll vote.

We propose that a possible direction for better prediction results would be to collect more points conducive to voter intention, such as stances on government policy. This would help with classification, as answering these questions would eventually lead respondents to the candidate that best fits their views. Prediction would likely be made easier as these issues likely drive voting tendencies more than demographics or location alone. We saw in our visual for #12 that states with higher minority percentages aren’t necessarily going to vote for the Democratic candidate, so basing prediction off of demographics can be detrimental to the analysis.


\newpage
## Codes
```{r appendix, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```